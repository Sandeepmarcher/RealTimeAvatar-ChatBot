{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efb808-9445-4cad-8f29-ddd3530794f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "# Paths\n",
    "WAV2LIP_PATH = \"Wav2Lip\"\n",
    "INPUT_IMAGE = \"inputs/sample.webp\"\n",
    "OUTPUT_VIDEO = \"outputs/output.mp4\"\n",
    "AUDIO_FILE = \"inputs/audio.wav\"\n",
    "\n",
    "# Function to run Wav2Lip inference\n",
    "def run_wav2lip(audio_path, img_path, output_path):\n",
    "    command = f\"python {WAV2LIP_PATH}/inference.py --checkpoint_path {WAV2LIP_PATH}/Wav2Lip.pth --face {img_path} --audio {audio_path} --outfile {output_path}\"\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "# Function to capture audio and save as audio.wav\n",
    "def record_audio(audio_path, duration=5):\n",
    "    import sounddevice as sd\n",
    "    from scipy.io.wavfile import write\n",
    "    \n",
    "    fs = 44100  # Sampling frequency\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=2, dtype='int16')\n",
    "    sd.wait()\n",
    "    write(audio_path, fs, audio_data)\n",
    "    print(f\"Audio saved at {audio_path}\")\n",
    "\n",
    "# Main loop to capture real-time video and convert to avatar\n",
    "def real_time_avatar():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Camera not accessible!\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video.\")\n",
    "            break\n",
    "\n",
    "        # Show real-time camera feed\n",
    "        cv2.imshow('Real-Time Avatar', frame)\n",
    "\n",
    "        # Press 'r' to record and generate avatar lip-sync\n",
    "        if cv2.waitKey(1) & 0xFF == ord('r'):\n",
    "            record_audio(AUDIO_FILE)\n",
    "            \n",
    "            # Run Wav2Lip\n",
    "            run_wav2lip(AUDIO_FILE, INPUT_IMAGE, OUTPUT_VIDEO)\n",
    "            \n",
    "            # Play the generated avatar video\n",
    "            play_video(OUTPUT_VIDEO)\n",
    "        \n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Function to play the generated avatar video\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Avatar Output', frame)\n",
    "        \n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_avatar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4efb6-a451-4d62-ba49-acb1cd7a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define input file paths\n",
    "image_path = \"Wav2Lip/inputs/sample.webp\"  # Path to your image file\n",
    "audio_path = \"Wav2Lip/inputs/harvard.wav\"   # Path to your audio file\n",
    "output_path = \"output.mp4\"  # Path for the generated video\n",
    "\n",
    "# Command to run Wav2Lip\n",
    "command = f\"python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face {image_path} --audio {audio_path} --outfile {output_path}\"\n",
    "\n",
    "# Run the command\n",
    "os.system(command)\n",
    "\n",
    "# Check if the output file is generated\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Video generated successfully: {output_path}\")\n",
    "else:\n",
    "    print(\"Failed to generate video. Check your input files or model path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cead3e8-bc01-490a-b948-bd5e2f29a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "# Define the model and prompt\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if using GPT-4\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6206aa-dde6-48ad-8ca0-7f0c79f23ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
